<!DOCTYPE html>
<html>
<head>
	<meta name="keywords" content="Thanasis Pittas,  Graduate Student, Computer Science, Machine Learning, UW-Madison, UW, Robust Statistics, Ilias Diakonikolas,">
	<meta name="google-site-verification" content="2Zixfk_FGYd5AYNhRWmK1jtwRxRlJAuyKM85sQGXEhQ" />
	<meta name="description" content="I am a PhD student in the CS department at UW-Madison, advised by Ilias Diakonikolas. I am working on theoretical machine learning and robust statistics.">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<!-- <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> -->
	<!-- <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,300;0,400;1,300&display=swap" rel="stylesheet"> -->
	<!-- <link href="https://fonts.googleapis.com/css2?family=Roboto&family=Roboto+Mono&display=swap" rel="stylesheet"> -->
	<!-- <link href='https://fonts.googleapis.com/css?family=Roboto:light,regular,medium,thin,italic,mediumitalic,bold' rel='stylesheet' type='text/css'> -->
	<link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;1,100;1,300;1,400;1,500;1,700&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono&display=swap" rel="stylesheet">
</head>
<script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
<!-- <title>Thanasis Pittas</title> -->


<style type="text/css">
	html {	background: rgb(251, 251, 251); }

	body{
	margin:20px auto;
	max-width:800px;
	line-height:1.6;
	font-size:15px;
	/*font-family: 'Noto Sans', sans-serif;*/
	font-family: 'Roboto', sans-serif;
	font-weight: 400;
	color:#000;
	padding:10px
	}

	h1,h2,h3,h4,h5,h6 {
	font-family: Roboto, sans-serif;
	line-height: 1.2;
	/*color: #330; */
	font-weight: 400;
	margin: 0px;
	/* margin-top: 10px; */
	}

	

	.multiple {
    display: flex;
    align-items: center;
    justify-content: space-between;
    /* height: 500px; */
  	}
	.text {
    width: 50%;
    text-align: center;
  	}
	.image {
    width: 50%;
    height: auto;
    display: flex;
    align-items: center;
    justify-content: center;
	margin-top: 5px;
	margin-bottom: 5px;
	margin-left: 35px;
	margin-right: 10px;
  }

	var {
	font-family: "Roboto Mono";
	font-size: 14.5px;
	font-style: normal;

	}

	b{
		font-family: 'Roboto', sans-serif;
		font-weight: 500;
	}

	.label:after{
		content:'Thanasis Pittas';
	}
	.label:hover:after{
		content:'Θανάσης Πήττας';
	}

	a{
		cursor: pointer;
		text-decoration: none;
		color: #9b0000;
	}

	div.abstract {
		font-size: 13px;
	}

	li{
	margin: 20px 0;
	}

	/* Styles for the desktop version */
	@media (min-width: 768px) {
		.desktop-version {
			display: block;
		}

		.mobile-version {
			display: none;
		}

		/* Fancier style for the text boxes in desktop mode */
		.container {
			background: white;
			position: relative;
			max-width: 100%;
			/* max-width: 960px; */
			/* min-width: 600px; */
			padding: 20px 20px;
			margin: 25px auto;
			box-shadow: 0 0 10px rgba(50, 50, 50, 0.08);
			border-radius: 6px;
			/* border-radius: 4px; */
			/* display: flex; */
			/* flex-wrap: wrap-reverse; */
			align-items: flex-end; 
		}
	}

	/* Styles for the mobile version */
	@media (max-width: 767px) {
		.desktop-version {
			display: none;
		}

		.mobile-version {
			display: block;
		}
		.container{}
	}
	
</style>


<body>

<h1><span class="label"></span></h1>

<div class="desktop-version">


	<div  class="container multiple"  style="padding-top:5px; padding-bottom:5px; display: flex; align-items: center;">
		<!-- <div id="content-desktop">
			<img src="image_comp.jpg" style="border-radius: 28px; float: right; padding: 10px 15px; margin:20px auto;" image-rendering="pixelated" width="320px">
		</div>


		<div id="content-mobile">
		<center><img  src="image_comp.jpg" style="border-radius: 27px;  padding: 15px;" image-rendering="pixelated" width="320px"></center>
		</div> -->

		<div>
			<p>I am a PhD student in the <a href="https://www.cs.wisc.edu/">Computer Sciences Department</a> at the <a href="https://www.wisc.edu/">University of Wisconsin–Madison</a>. I am fortunate to be advised by Prof. <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>. I am working on theoretical machine learning and robust statistics. </p>
			<p>Before coming to Madison, I did my undergraduate studies in Greece, at the <a href="https://www.ece.ntua.gr/en">National Technical University of Athens</a>.</p>
			<p>Feel free to contact me at: <var>pittas[at]wisc.edu</var> or <var>pittas.than[at]gmail.com</var>.</p>
		</div>

		<div class="image">
			<img src="image_comp.jpg"  style="border-radius: 7px;" image-rendering="pixelated" width="320px"  >
		</div>
	</div>
</div>

<div  class="mobile-version" style="padding-top:0px; padding-bottom:5px;">
	 <!-- <div id="content-desktop">
		<img src="image_comp.jpg" style="border-radius: 28px; float: right; padding: 10px 15px; margin:20px auto;" image-rendering="pixelated" width="320px">
	</div> -->


	<div>
	<center><img  src="image_comp.jpg" style="border-radius: 25px;  padding: 15px;" image-rendering="pixelated" width="320px"></center>
	</div>

	<div class="container">
		<p><br>I am a PhD student in the <a href="https://www.cs.wisc.edu/">Computer Sciences Department</a> at the <a href="https://www.wisc.edu/">University of Wisconsin–Madison</a>. I am fortunate to be advised by Prof. <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>. I am working on theoretical machine learning and robust statistics. </p>
		<p>Before coming to Madison, I did my undergraduate studies in Greece, at the <a href="https://www.ece.ntua.gr/en">National Technical University of Athens</a>.</p>
		<p>Feel free to contact me at: <var>pittas[at]wisc.edu</var> or <var>pittas.than[at]gmail.com</var>.</p>
	</div>
	
</div>


<div class="container">
	<h2 style="clear: both;">Publications</h2>

	<ul style="list-style-type: none">

		<li><b>Clustering Mixtures of Bounded Covariance Distributions Under Optimal Separation</b>
			<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, and <a href="https://ankitp.net/">Jasper C.H. Lee</a>
			<br> <i> Manuscript, 2023</i> 
			<br> [<a onclick= "toggle('paper11abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2312.11769" >arXiv</a>]<div id = 'paper11abs' class= 'abstract' style= ' display:none'  >We study the clustering problem for mixtures of bounded covariance distributions, under a fine-grained separation assumption. Specifically, given samples from a $k$-component mixture distribution $D = \sum_{i =1}^k w_i P_i$, where each $w_i \ge \alpha$ for some known parameter $\alpha$, and each $P_i$ has unknown covariance $\Sigma_i \preceq \sigma^2_i \cdot I_d$ for some unknown $\sigma_i$, the goal is to cluster the samples assuming a pairwise mean separation in the order of $(\sigma_i+\sigma_j)/\sqrt{\alpha}$ between every pair of components $P_i$ and $P_j$. Our contributions are as follows: For the special case of nearly uniform mixtures, we give the first poly-time algorithm for this clustering task. Prior work either required separation scaling with the maximum cluster standard deviation (i.e. $\max_i \sigma_i$) [DKK+22b] or required both additional structural assumptions and mean separation scaling as a large degree polynomial in $1/\alpha$ [BKK22]. For general-weight mixtures, we point out that accurate clustering is information-theoretically impossible under our fine-grained mean separation assumptions. We introduce the notion of a clustering refinement -- a list of not-too-small subsets satisfying a similar separation, and which can be merged into a clustering approximating the ground truth -- and show that it is possible to efficiently compute an accurate clustering refinement of the samples. Furthermore, under a variant of the "no large sub-cluster'' condition from in prior work [BKK22], we show that our algorithm outputs an accurate clustering, not just a refinement, even for general-weight mixtures. As a corollary, we obtain efficient clustering algorithms for mixtures of well-conditioned high-dimensional log-concave distributions. Moreover, our algorithm is robust to $\Omega(\alpha)$-fraction of adversarial outliers.
		</div> 
		</li>

		<li><b>Near-Optimal Algorithms for Gaussians with Huber Contamination: Mean Estimation and Linear Regression</b>
			<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, and <a href="https://ankitp.net/">Ankit Pensia</a>
			<br> <i> Advances in Neural Information Processing Systems (NeurIPS), 2023</i> 
			<br> [<a onclick= "toggle('paper10abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2312.01547" >arXiv</a>]<div id = 'paper10abs' class= 'abstract' style= ' display:none'  >We study the fundamental problems of Gaussian mean estimation and linear regression with Gaussian covariates in the presence of Huber contamination. Our main contribution is the design of the first sample near-optimal and almost linear-time algorithms with optimal error guarantees for both these problems. Specifically, for Gaussian robust mean estimation on $\mathbb{R}^d$ with contamination parameter $\epsilon \in (0, \epsilon_0)$ for a small absolute constant $\epsilon_0$, we give an algorithm with sample complexity $n = \tilde{O}(d/\epsilon^2)$ and almost linear runtime that approximates the target mean within $\ell_2$-error $O(\epsilon)$. This improves on prior work that achieved this error guarantee with polynomially suboptimal sample and time complexity. For robust linear regression, we give the first algorithm with sample complexity $n = \tilde{O}(d/\epsilon^2)$ and almost linear runtime that approximates the target regressor within $\ell_2$-error $O(\epsilon)$. This is the first polynomial sample and time algorithm achieving the optimal error guarantee, answering an open question in the literature. At the technical level, we develop a methodology that yields almost-linear time algorithms for multi-directional filtering that may be of broader interest. 
		</div> 
		</li>

		<li><b>A Spectral Algorithm for List-Decodable Covariance Estimation in Relative Frobenius Norm</b>
			<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, <a href="https://jasperchlee.github.io">Jasper C. H. Lee</a>, and <a href="https://ankitp.net/">Ankit Pensia</a>
			<br> <i> Advances in Neural Information Processing Systems (NeurIPS), 2023</i> 
			<br> <b>Spotlight Presentation</b>
			<br> [<a onclick= "toggle('paper8abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2305.00966" >arXiv</a>]<div id = 'paper8abs' class= 'abstract' style= ' display:none'  >We study the problem of list-decodable Gaussian covariance estimation. Given a multiset $T$ of $n$ points in $\mathbb{R}^d$ such that an unknown $\alpha &lt; 1/2$ fraction of points in $T$ are i.i.d. samples from an unknown Gaussian $\mathcal{N}(\mu,\Sigma)$, the goal is to output a list of $O(1/\alpha)$ hypotheses at least one of which is close to $\Sigma$ in relative Frobenius norm. Our main result is a $\mathrm{poly}(d,1/\alpha)$ sample and time algorithm for this task that guarantees relative Frobenius norm error of $\mathrm{poly}(1/\alpha)$. Importantly, our algorithm relies purely on spectral techniques. As a corollary, we obtain an efficient spectral algorithm for robust partial clustering of Gaussian mixture models (GMMs) -- a key ingredient in the recent work of [BDJ+22] on robustly learning arbitrary GMMs. Combined with the other components of [BDJ+22], our new method yields the first Sum-of-Squares-free algorithm for robustly learning GMMs. At the technical level, we develop a novel multi-filtering method for list-decodable covariance estimation that may be useful in other settings.
		</div> 
		</li>


		<li><b>SQ Lower Bounds for Learning Bounded Covariance GMMs</b>
			<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, and <a href="https://nikoszarifis.github.io/">Nikos Zarifis</a>
			<br> <i> Conference on Learning Theory (COLT), 2023</i> 
			<br> [<a onclick= "toggle('paper9abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2306.13057" >arXiv</a>]<div id = 'paper9abs' class= 'abstract' style= ' display:none'  >We study the complexity of learning 
				mixtures of separated Gaussians with common unknown bounded covariance matrix. Specifically, we focus on learning Gaussian mixture models (GMMs) on $\mathbb{R}^d$ of the form $P= \sum_{i=1}^k w_i \mathcal{N}(\boldsymbol \mu_i,\boldsymbol \Sigma_i)$, where $\boldsymbol \Sigma_i = \boldsymbol \Sigma \preceq \boldsymbol I$ and $\min_{i \neq j} \|\boldsymbol \mu_i - \boldsymbol \mu_j\|_2 \geq k^\epsilon$ for some $\epsilon>0$. Known learning algorithms for this family of GMMs have complexity $(dk)^{O(1/\epsilon)}$. In this work, we prove that any Statistical Query (SQ) algorithm for this problem requires complexity at least $d^{\Omega(1/\epsilon)}$. In the special case where the separation is on the order of $k^{1/2}$, we additionally obtain fine-grained SQ lower bounds with the correct exponent. Our SQ lower bounds imply similar lower bounds for low-degree polynomial tests. Conceptually, our results provide evidence that known algorithms for this problem are nearly best possible. 
		</div> 
		</li>

		
		<li><b>Nearly-Linear Time and Streaming Algorithms for Outlier-Robust PCA </b>
			<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, and <a href="https://ankitp.net/">Ankit Pensia</a>
			<br> <i> International Conference on Machine Learning (ICML), 2023</i>  
			<br> [<a onclick= "toggle('paper7abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2305.02544" >arXiv</a>]<div id = 'paper7abs' class= 'abstract' style= ' display:none'  >We study principal component analysis (PCA), where given a dataset in $\mathbb{R}^d$ from a distribution, the task is to find a unit vector $v$ that approximately maximizes the variance of the distribution after being projected along $v$. Despite being a classical task, standard estimators fail drastically if the data contains even a small fraction  of outliers, motivating the problem of robust PCA. Recent work has developed computationally-efficient algorithms for robust PCA that either take super-linear time or have sub-optimal error guarantees. Our main contribution is to develop a nearly linear time algorithm for robust PCA with near-optimal error guarantees. We also develop a single-pass streaming algorithm for robust PCA with memory usage  nearly-linear in the dimension.
		</div> 
		</li>


		<li><b>List-Decodable Sparse Mean Estimation via Difference-of-Pairs Filtering</b>
		<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, <a href="https://sushrutk.github.io/">Sushrut Karmalkar</a>, and <a href="https://ankitp.net/">Ankit Pensia</a>
		<br> <i> Advances in Neural Information Processing Systems (NeurIPS), 2022</i> 
		<br> <b>Oral Presentation</b>
		<br> [<a onclick= "toggle('paper6abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2206.05245" >arXiv</a>]<div id = 'paper6abs' class= 'abstract' style= ' display:none'  >We study the problem of list-decodable sparse mean estimation. Specifically, for a parameter $\alpha \in (0, 1/2)$, we are given $m$ points in $\mathbb{R}^n$, 
	$\lfloor \alpha m \rfloor$ of which are i.i.d. samples from a distribution $D$ with unknown $k$-sparse mean $\mu$. No assumptions are made on the remaining points, which form the majority of the dataset. The goal is to return a small list of candidates containing a vector $\hat \mu$ such that $\| \hat \mu - \mu \|_2$ is small. Prior work had studied the problem of list-decodable mean estimation in the dense setting. In this work, we develop a novel, conceptually simpler technique for list-decodable mean estimation. As the main application of our approach, we provide the first sample and computationally efficient algorithm for list-decodable sparse mean estimation. In particular, for distributions with  
	''certifiably bounded'' $t$-th moments in $k$-sparse directions and sufficiently light tails, our algorithm achieves error of $(1/\alpha)^{O(1/t)}$ with sample complexity $m = (k\log(n))^{O(t)}/\alpha$ and running time $\mathrm{poly}(mn^t)$. For the special case of Gaussian inliers, our algorithm achieves the optimal error guarantee of $\Theta (\sqrt{\log(1/\alpha)})$ with quasi-polynomial sample and computational complexity. We complement our upper bounds with nearly-matching statistical query and low-degree polynomial testing lower bounds. 
	</div> 
		</li>

		<li><b>Robust Sparse Mean Estimation via Sum of Squares</b>
		<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, <a href="https://sushrutk.github.io/">Sushrut Karmalkar</a>, and <a href="https://ankitp.net/">Ankit Pensia</a>
		<br> <i> Conference on Learning Theory (COLT), 2022</i> 
		<br> [<a onclick= "toggle('paper5abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2206 conference version.03441" >arXiv</a>]<div id = 'paper5abs' class= 'abstract' style= ' display:none'  >We study the problem of high-dimensional sparse mean estimation in the presence of an $\epsilon$-fraction of adversarial outliers. Prior work obtained sample and computationally efficient algorithms for this task for identity-covariance subgaussian distributions. In this work, we develop the first efficient algorithms for robust sparse mean estimation without a priori knowledge of the covariance. For distributions on $\mathbb{R}^d$ with `certifiably bounded' $t$-th moments and sufficiently light tails, our algorithm achieves error of $O(\epsilon^{1-1/t})$ with sample complexity $m = (k\log(d))^{O(t)}/\epsilon^{2-2/t}$. For the special case of the Gaussian distribution, our algorithm achieves near-optimal error of $\tilde O(\epsilon)$ with sample complexity $m = O(k^4 \mathrm{polylog}(d))/\epsilon^2$. Our algorithms follow the Sum-of-Squares based proofs to algorithms approach. We complement our upper bounds with Statistical Query and low-degree polynomial testing lower bounds, providing evidence that the sample-time-error tradeoffs achieved by our algorithms are qualitatively best possible.</div> 
		</li>

		<li><b>Streaming Algorithms for High-Dimensional Robust Statistics</b>
		<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, and <a href="https://ankitp.net/">Ankit Pensia</a>
		<br> <i> International Conference on Machine Learning (ICML), 2022</i> 
		<br> [<a onclick= "toggle('paper4abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2204.12399" >arXiv</a>]<div id = 'paper4abs' class= 'abstract' style= ' display:none'  >We study high-dimensional robust statistics tasks in the streaming model. A recent line of work obtained computationally efficient algorithms for a range of high-dimensional robust estimation tasks. Unfortunately, all previous algorithms require storing the entire dataset, incurring memory at least quadratic in the dimension. In this work, we develop the first efficient streaming algorithms for high-dimensional robust statistics with near-optimal memory requirements (up to logarithmic factors). Our main result is for the task of high-dimensional robust mean estimation in (a strengthening of) Huber's contamination model. We give an efficient single-pass streaming algorithm for this task with near-optimal error guarantees and space complexity nearly-linear in the dimension. As a corollary, we obtain streaming algorithms with near-optimal space complexity for several more complex tasks, including robust covariance estimation, robust regression, and more generally robust stochastic optimization.</div> 
		</li>

		<li><b>Statistical Query Lower Bounds for List-Decodable Linear Regression</b>
		<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, <a href="https://ankitp.net/">Ankit Pensia</a>, and <a href="http://www.alistair-stewart.com/">Alistair Stewart</a>
		<br> <i> Advances in Neural Information Processing Systems (NeurIPS), 2021</i> 
		<br> <b>Spotlight Presentation</b>
		<br> [<a onclick= "toggle('paper3abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2106.09689" >arXiv</a>]<div id = 'paper3abs' class= 'abstract' style= ' display:none'  >We study the problem of list-decodable linear regression, where an adversary can corrupt a majority of the examples. Specifically, we are given a set $T$ of labeled examples $(x, y) \in \mathbb{R}^d \times \mathbb{R}$ and a parameter $0&lt;\alpha &lt; 1/2$ such that an $\alpha$-fraction of the points in $T$ are i.i.d. samples from a linear regression model with Gaussian covariates, and the remaining $(1-\alpha)$-fraction of the points are drawn from an arbitrary noise distribution. The goal is to output a small list of hypothesis vectors such that at least one of them is close to the target regression vector. Our main result is a Statistical Query (SQ) lower bound of $d^{\mathrm{poly}(1/\alpha)}$ for this problem. Our SQ lower bound qualitatively matches the performance of previously developed algorithms, providing evidence that current upper bounds for this task are nearly best possible.</div> 
		</li>


		<li><b>The Optimality of Polynomial Regression for Agnostic Learning under Gaussian Marginals</b>
		<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, and <a href="https://nikoszarifis.github.io/">Nikos Zarifis</a>
		<br> <i> Conference on Learning Theory (COLT), 2021</i>
		<br> [<a onclick= "toggle('paper2abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2102.04401">arXiv</a>]
		<div id = 'paper2abs' class= 'abstract' style= ' display:none' > We study the problem of agnostic learning under the Gaussian distribution. We develop a method for finding hard families of examples for a wide class of problems by using LP duality. For Boolean-valued concept classes, we show that the $L^1$-regression algorithm is essentially best possible, and therefore that the computational difficulty of agnostically learning a concept class is closely related to the polynomial degree required to approximate any function from the class in $L^1$-norm. Using this characterization along with additional analytic tools, we obtain optimal SQ lower bounds for agnostically learning linear threshold functions and the first non-trivial SQ lower bounds for polynomial threshold functions and intersections of halfspaces. We also develop an analogous theory for agnostically learning real-valued functions, and as an application prove near-optimal SQ lower bounds for agnostically learning ReLUs and sigmoids.  </div> 
		</li>

		<li><b>Estimating the Number of Induced Subgraphs from Incomplete Data and Neighborhood Queries</b>
		<br> with <a href="https://www.softlab.ntua.gr/~fotakis/">Dimitris Fotakis</a> and <a href="http://www.corelab.ntua.gr/~sskoul/">Stratis Skoulakis</a>
		<br> <i> AAAI Conference on Artificial Intelligence, 2021</i>
		<br> [<a onclick= "toggle('paper1abs')">Abstract</a>] [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/16525">Conference version</a>]
		<div id = 'paper1abs' class= 'abstract' style= ' display:none' > We consider a natural setting where network parameters are estimated from noisy and incomplete information about the network. More specifically, we investigate how we can efficiently estimate the number of small subgraphs (e.g., edges, triangles, etc.) based on full access to one or two noisy and incomplete samples of a large underlying network and on few queries revealing the neighborhood of carefully selected vertices. After specifying a random generator which removes edges from the underlying graph, we present estimators with strong provable performance guarantees, which exploit information from the noisy network samples and query a constant number of the most important vertices for the estimation. Our experimental evaluation shows that, in practice, a single noisy network sample and a couple of hundreds neighborhood queries suffice for accurately estimating the number of triangles in networks with millions of vertices and edges.  </div> 
		</li>
	</ul>
</div>


<div class="container">
	</header ><h2 style="clear: both;">Teaching and Service</h2>
	<ul style="list-style-type: none">
		<li style="margin: 0px">Grader for Introduction to Computational Learning Theory, UW-Madison, 2023</li>
		<li style="margin: 0px">Teaching Assistant for Introduction to Artificial Inteligence, UW-Madison, 2022</li>
		<li style="margin: 0px">Teaching Assistant for Discrete Mathematics, NTUA, 2019</li>
		<li style="margin: 0px">Teaching Assistant for Algorithms and Complexity, NTUA, 2019</li>
		<li style="margin: 0px">Lab Assistant for Introduction to Programming, NTUA, 2019</li>
		<li style="margin: 10px 0"><b>Reviewer:</b> NeurIPS 2023, ICLR 2023, NeurIPS 2022, ICML 2022</li>
	</ul>
</div>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-77EV6DC24Q"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"> </script>
<script>

	function toggle(id) {
		console.log(id)
		var e = document.getElementById(id);
		if(e.style.display == 'block')
			e.style.display = 'none';
		else
			e.style.display = 'block';
	};


    window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
  	gtag('js', new Date());
  	gtag('config', 'G-77EV6DC24Q'); 

    $.get("https://ipinfo.io", function(response) {
           	  const formatter = new Intl.DateTimeFormat('sv-SE', { year: '2-digit', month: '2-digit', day: '2-digit', hour: '2-digit', minute: '2-digit', second: '2-digit', timeZone:  'America/Chicago', timeZoneName: 'short'});
			  response.date = formatter.format(new Date());
			  gtag('event', 'myevent', {"mycity": response.city, "mycountry": response.country, "myregion": response.region, "myloc": response.loc, "mydate": response.date, "myadr": response.ip});
        }, "json")
</script>