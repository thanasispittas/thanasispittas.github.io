<!DOCTYPE html>
<html>
<head>
<meta name="keywords" content="Thanasis Pittas,  Graduate Student, Computer Science, Machine Learning, UW-Madison, UW, Robust Statistics, Ilias Diakonikolas,">
<meta name="google-site-verification" content="2Zixfk_FGYd5AYNhRWmK1jtwRxRlJAuyKM85sQGXEhQ" />
<meta name="description" content="I am a PhD student in the CS department at UW-Madison, advised by Ilias Diakonikolas. I am working on theoretical machine learning and robust statistics.">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="preconnect" href="https://fonts.googleapis.com">
<!-- <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> -->
<!-- <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,300;0,400;1,300&display=swap" rel="stylesheet"> -->
<link href="https://fonts.googleapis.com/css2?family=Roboto&family=Roboto+Mono&display=swap" rel="stylesheet">
<script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>
<title>Thanasis Pittas</title>

<script type="text/javascript">
                
    function toggle(id) {
      console.log(id)
       var e = document.getElementById(id);
       if(e.style.display == 'block')
          e.style.display = 'none';
       else
          e.style.display = 'block';
   };
</script>

<style type="text/css">

h1,h2,h3,h4,h5,h6 {
  font-family: Roboto, sans-serif;
  line-height: 1.2;
  /* color: #330; */
  font-weight: 100;
  margin-bottom: 15px;
  margin-top: 20px;
}


body{
margin:20px auto;
max-width:800px;
line-height:1.6;
font-size:15px;
/*font-family: 'Noto Sans', sans-serif;*/
font-family: 'Roboto', sans-serif;
color:#000;
padding:10px
}

var {
  font-family: "Roboto Mono";
  font-size:15px;
  font-style: normal;

}

.label:after{
    content:'Thanasis Pittas';
}
.label:hover:after{
    content:'Θανάσης Πήττας';
}

a{
	cursor: pointer;
	text-decoration: none;
	color: #9b0000;
}
a.abstractlink{
	/*cursor: pointer;
	text-decoration: none;*/
	color: #9b0000;
}

div.abstract {
	font-size: 13px;
}

li{
  margin: 20px 0;
}

#content-desktop {display: block;}
#content-mobile {display: none;}

@media screen and (max-width: 768px) {

#content-desktop {display: none;}
#content-mobile {display: block;}

}

</style>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-77EV6DC24Q"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"> </script>
<script>
    window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
  	gtag('js', new Date());
  	gtag('config', 'G-77EV6DC24Q'); 

    $.get("https://ipinfo.io", function(response) {
           	  const formatter = new Intl.DateTimeFormat('sv-SE', { year: '2-digit', month: '2-digit', day: '2-digit', hour: '2-digit', minute: '2-digit', second: '2-digit', timeZone:  'America/Chicago', timeZoneName: 'short'});
			  response.date = formatter.format(new Date());
			  gtag('event', 'myevent', {"mycity": response.city, "mycountry": response.country, "myregion": response.region, "myloc": response.loc, "mydate": response.date, "myadr": response.ip});
        }, "json")
</script>
</head>

<body>

<h1><span class="label"></span></h1>

<div id="content-desktop">
	<img src="image.jpg" style="border-radius: 27px; float: right; padding: 15px;">
</div>


<div id="content-mobile">
<center><img  src="image.jpg" style="border-radius: 27px;  padding: 15px;"></center>
</div>


<p><br>I am a PhD student in the <a href="https://www.cs.wisc.edu/">Computer Sciences Department</a> at the University of Wisconsin–Madison. I am fortunate to be advised by Prof. <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>. I am working on theoretical machine learning and robust statistics. </p>

	<p>Before coming to Madison, I did my undergraduate studies in Greece, at the <a href="https://www.ece.ntua.gr/en">National Technical University of Athens</a>.</p>

	<p>Feel free to contact me at: <var>pittas[at]wisc.edu</var> or <var>pittas.than[at]gmail.com</var>.</p>

<h2 style="clear: both;">Publications</h2>
<ul style="list-style-type: none">

	<li><b>Robust Sparse Mean Estimation via Sum of Squares</b>
	<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, <a href="https://sushrutk.github.io/">Sushrut Karmalkar</a>, and <a href="https://ankitp.net/">Ankit Pensia</a>
	<br> <i> Conference on Learning Theory (COLT), 2022 (To appear)</i> 
	<br> [<a class= 'abstractlink' onclick= "toggle('paper5abs')">Abstract</a>]<div id = 'paper5abs' class= 'abstract' style= ' display:none'  >We study the problem of high-dimensional sparse mean estimation in the presence of an $\epsilon$-fraction of adversarial outliers. Prior work obtained sample and computationally efficient algorithms for this task for identity-covariance subgaussian distributions. In this work, we develop the first efficient algorithms for robust sparse mean estimation without a priori knowledge of the covariance. For distributions on $\mathbb{R}^d$ with `certifiably bounded' $t$-th moments and sufficiently light tails, our algorithm achieves error of $O(\epsilon^{1-1/t})$ with sample complexity $m = (k\log(d))^{O(t)}/\epsilon^{2-2/t}$. For the special case of the Gaussian distribution, our algorithm achieves near-optimal error of $\tilde O(\epsilon)$ with sample complexity $m = O(k^4 \mathrm{polylog}(d))/\epsilon^2$. Our algorithms follow the Sum-of-Squares based proofs to algorithms approach. We complement our upper bounds with Statistical Query and low-degree polynomial testing lower bounds, providing evidence that the sample-time-error tradeoffs achieved by our algorithms are qualitatively best possible.</div> 
	</li>

	<li><b>Streaming Algorithms for High-Dimensional Robust Statistics</b>
	<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, and <a href="https://ankitp.net/">Ankit Pensia</a>
	<br> <i> International Conference on Machine Learning (ICML), 2022 (To appear)</i> 
	<br> [<a class= 'abstractlink' onclick= "toggle('paper4abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2204.12399" >arXiv</a>]<div id = 'paper4abs' class= 'abstract' style= ' display:none'  >We study high-dimensional robust statistics tasks in the streaming model. A recent line of work obtained computationally efficient algorithms for a range of high-dimensional robust estimation tasks. Unfortunately, all previous algorithms require storing the entire dataset, incurring memory at least quadratic in the dimension. In this work, we develop the first efficient streaming algorithms for high-dimensional robust statistics with near-optimal memory requirements (up to logarithmic factors). Our main result is for the task of high-dimensional robust mean estimation in (a strengthening of) Huber's contamination model. We give an efficient single-pass streaming algorithm for this task with near-optimal error guarantees and space complexity nearly-linear in the dimension. As a corollary, we obtain streaming algorithms with near-optimal space complexity for several more complex tasks, including robust covariance estimation, robust regression, and more generally robust stochastic optimization.</div> 
	</li>

	<li><b>Statistical Query Lower Bounds for List-Decodable Linear Regression</b>
	<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, <a href="https://ankitp.net/">Ankit Pensia</a>, and <a href="http://www.alistair-stewart.com/">Alistair Stewart</a>
	<br> <i> Advances in Neural Information Processing Systems (NeurIPS), 2021</i> 
	<br> <b>Spotlight Presentation</b>
	<br> [<a class= 'abstractlink' onclick= "toggle('paper3abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2106.09689" >arXiv</a>]
	 [<a href="https://proceedings.neurips.cc/paper/2021/hash/19b1b73d63d4c9ea79f8ca57e9d67095-Abstract.html">Conference version</a>]<div id = 'paper3abs' class= 'abstract' style= ' display:none'  >We study the problem of list-decodable linear regression, where an adversary can corrupt a majority of the examples. Specifically, we are given a set $T$ of labeled examples $(x, y) \in \mathbb{R}^d \times \mathbb{R}$ and a parameter $0< \alpha &lt; 1/2$ such that an $\alpha$-fraction of the points in $T$ are i.i.d. samples from a linear regression model with Gaussian covariates, and the remaining $(1-\alpha)$-fraction of the points are drawn from an arbitrary noise distribution. The goal is to output a small list of hypothesis vectors such that at least one of them is close to the target regression vector. Our main result is a Statistical Query (SQ) lower bound of $d^{\mathrm{poly}(1/\alpha)}$ for this problem. Our SQ lower bound qualitatively matches the performance of previously developed algorithms, providing evidence that current upper bounds for this task are nearly best possible.</div> 
	</li>


	<li><b>The Optimality of Polynomial Regression for Agnostic Learning under Gaussian Marginals</b>
	<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, and <a href="https://nikoszarifis.github.io/">Nikos Zarifis</a>
	<br> <i> Conference on Learning Theory (COLT), 2021</i>
	<br> [<a class= 'abstractlink' onclick= "toggle('paper2abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2102.04401">arXiv</a>] [<a href="https://proceedings.mlr.press/v134/diakonikolas21c.html">Conference version</a>]
	<div id = 'paper2abs' class= 'abstract' style= ' display:none' > We study the problem of agnostic learning under the Gaussian distribution. We develop a method for finding hard families of examples for a wide class of problems by using LP duality. For Boolean-valued concept classes, we show that the $L^1$-regression algorithm is essentially best possible, and therefore that the computational difficulty of agnostically learning a concept class is closely related to the polynomial degree required to approximate any function from the class in $L^1$-norm. Using this characterization along with additional analytic tools, we obtain optimal SQ lower bounds for agnostically learning linear threshold functions and the first non-trivial SQ lower bounds for polynomial threshold functions and intersections of halfspaces. We also develop an analogous theory for agnostically learning real-valued functions, and as an application prove near-optimal SQ lower bounds for agnostically learning ReLUs and sigmoids.  </div> 
	</li>

	<li><b>Estimating the Number of Induced Subgraphs from Incomplete Data and Neighborhood Queries</b>
	<br> with <a href="https://www.softlab.ntua.gr/~fotakis/">Dimitris Fotakis</a> and <a href="http://www.corelab.ntua.gr/~sskoul/">Stratis Skoulakis</a>
	<br> <i> AAAI Conference on Artificial Intelligence, 2021</i>
	<br> [<a class= 'abstractlink' onclick= "toggle('paper1abs')">Abstract</a>] [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/16525">Conference version</a>]
	<div id = 'paper1abs' class= 'abstract' style= ' display:none' > We consider a natural setting where network parameters are estimated from noisy and incomplete information about the network. More specifically, we investigate how we can efficiently estimate the number of small subgraphs (e.g., edges, triangles, etc.) based on full access to one or two noisy and incomplete samples of a large underlying network and on few queries revealing the neighborhood of carefully selected vertices. After specifying a random generator which removes edges from the underlying graph, we present estimators with strong provable performance guarantees, which exploit information from the noisy network samples and query a constant number of the most important vertices for the estimation. Our experimental evaluation shows that, in practice, a single noisy network sample and a couple of hundreds neighborhood queries suffice for accurately estimating the number of triangles in networks with millions of vertices and edges.  </div> 
	</li>
</ul>


</header ><h2 style="clear: both;">Teaching and Service</h2>
<ul style="list-style-type: none">
	<li style="margin: 0px">Teaching Assistant for Introduction to Artificial Inteligence, UW-Madison, Spring 2022</li>
	<li style="margin: 0px">Teaching Assistant for Discrete Mathematics, NTUA, Spring 2019</li>
	<li style="margin: 0px">Teaching Assistant for Algorithms and Complexity, NTUA, Fall 2019</li>
	<li style="margin: 0px">Lab Assistant for Introduction to Programming, NTUA, Fall 2019</li>
	<li style="margin: 10px 0"><b>Reviewer:</b> ICML 2022</li>
</ul>

