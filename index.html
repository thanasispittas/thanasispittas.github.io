<!DOCTYPE html>
<html>
<head>
	<meta name="keywords" content="Thanasis Pittas,  Graduate Student, Computer Science, Machine Learning, UW-Madison, UW, Robust Statistics, Ilias Diakonikolas,">
	<meta name="google-site-verification" content="2Zixfk_FGYd5AYNhRWmK1jtwRxRlJAuyKM85sQGXEhQ" />
	<meta name="description" content="I am a PhD student in the CS department at UW-Madison, advised by Ilias Diakonikolas. I am working on theoretical machine learning and robust statistics.">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<!-- <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> -->
	<!-- <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,300;0,400;1,300&display=swap" rel="stylesheet"> -->
	<!-- <link href="https://fonts.googleapis.com/css2?family=Roboto&family=Roboto+Mono&display=swap" rel="stylesheet"> -->
	<!-- <link href='https://fonts.googleapis.com/css?family=Roboto:light,regular,medium,thin,italic,mediumitalic,bold' rel='stylesheet' type='text/css'> -->
	<link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;1,100;1,300;1,400;1,500;1,700&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono&display=swap" rel="stylesheet">
</head>
<script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
<!-- <title>Thanasis Pittas</title> -->


<style type="text/css">
	html {	background: rgb(251, 251, 251); }

	body{
	margin:20px auto;
	max-width:900px;
	line-height:1.6;
	font-size:16px;
	/*font-family: 'Noto Sans', sans-serif;*/
	font-family: 'Arial', sans-serif;
	font-weight: 400;
	color:#000;
	padding:10px
	}

	h1,h2,h3,h4,h5,h6 {
	font-family: 'Arial', sans-serif;
	line-height: 1.2;
	/*color: #330; */
	font-weight: 400;
	margin: 0px;
	/* margin-top: 10px; */
	}

	

	.multiple {
    display: flex;
    align-items: center;
    justify-content: space-between;
    /* height: 500px; */
  	}
	.text {
    width: 50%;
    text-align: center;
  	}
	.image {
    width: 50%;
    height: auto;
    display: flex;
    align-items: center;
    justify-content: center;
	margin-top: 5px;
	margin-bottom: 5px;
	margin-left: 35px;
	margin-right: 10px;
  }

	var {
	font-family: "Roboto Mono";
	font-size: 14.5px;
	font-style: normal;

	}

	b{
		font-family: 'Roboto', sans-serif;
		font-weight: 500;
	}

	.label:after{
		content:'Thanasis Pittas';
	}
	.label:hover:after{
		content:'Θανάσης Πήττας';
	}

	a{
		cursor: pointer;
		text-decoration: none;
		color: #9b0000;
	}

	div.abstract {
		font-size: 13px;
	}

	li{
	margin: 20px 0;
	}

	/* Styles for the desktop version */
	@media (min-width: 768px) {
		.desktop-version {
			display: block;
		}

		.mobile-version {
			display: none;
		}

		/* Fancier style for the text boxes in desktop mode */
		.container {
			background: white;
			position: relative;
			max-width: 100%;
			/* max-width: 960px; */
			/* min-width: 600px; */
			padding: 20px 20px;
			margin: 25px auto;
			box-shadow: 0 0 10px rgba(50, 50, 50, 0.08);
			border-radius: 6px;
			/* border-radius: 4px; */
			/* display: flex; */
			/* flex-wrap: wrap-reverse; */
			align-items: flex-end; 
		}
	}

	/* Styles for the mobile version */
	@media (max-width: 767px) {
		.desktop-version {
			display: none;
		}

		.mobile-version {
			display: block;
		}
		.container{}
	}
	
</style>


<body>

<h1><span class="label"></span></h1>

<div class="desktop-version">


	<div  class="container multiple"  style="padding-top:5px; padding-bottom:5px; display: flex; align-items: center;">
		<!-- <div id="content-desktop">
			<img src="image_comp.jpg" style="border-radius: 28px; float: right; padding: 10px 15px; margin:20px auto;" image-rendering="pixelated" width="320px">
		</div>


		<div id="content-mobile">
		<center><img  src="image_comp.jpg" style="border-radius: 27px;  padding: 15px;" image-rendering="pixelated" width="320px"></center>
		</div> -->

		<div>
			<p>I am a PhD student in the <a href="https://www.cs.wisc.edu/">Computer Sciences Department</a> at the <a href="https://www.wisc.edu/">University of Wisconsin–Madison</a>. I am fortunate to be advised by Prof. <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>. I am working on theoretical machine learning and robust statistics. </p>
			<p>This summer, I will be interning at Amazon AWS.</p>
			<p>Before coming to Madison, I did my undergraduate studies in Greece, at the <a href="https://www.ece.ntua.gr/en">National Technical University of Athens</a>.</p>
			<p>My CV is available <a href="cv.pdf">here</a>.</p>
			<p>Feel free to contact me at <var>pittas[at]wisc.edu</var> or <var>pittas.than[at]gmail.com</var>.</p>
		</div>

		<div class="image">
			<img src="image_comp.jpg"  style="border-radius: 7px;" image-rendering="pixelated" width="390px"  >
		</div>
	</div>
</div>

<div  class="mobile-version" style="padding-top:0px; padding-bottom:5px;">
	 <!-- <div id="content-desktop">
		<img src="image_comp.jpg" style="border-radius: 28px; float: right; padding: 10px 15px; margin:20px auto;" image-rendering="pixelated" width="320px">
	</div> -->


	<div>
	<center><img  src="image_comp.jpg" style="border-radius: 25px;  padding: 15px;" image-rendering="pixelated" width="320px"></center>
	</div>

	<div>
		<p>I am a PhD student in the <a href="https://www.cs.wisc.edu/">Computer Sciences Department</a> at the <a href="https://www.wisc.edu/">University of Wisconsin–Madison</a>. I am fortunate to be advised by Prof. <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>. I am working on theoretical machine learning and robust statistics. </p>
			<p>Before coming to Madison, I did my undergraduate studies in Greece, at the <a href="https://www.ece.ntua.gr/en">National Technical University of Athens</a>.</p>
			<p>My CV is available <a href="cv.pdf">here</a>.</p>
			<p>Feel free to contact me at <var>pittas[at]wisc.edu</var> or <var>pittas.than[at]gmail.com</var>.</p>
	</div>
	
</div>


<div class="container">
	<h2 style="clear: both;">Publications</h2>

	<ul style="list-style-type: none">
		
		<li><b>On Fine-Grained Distinct Element Estimation</b>
			<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>, <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, <a href="https://jasperchlee.github.io">Jasper C.H. Lee</a>, <a href="https://www.cs.cmu.edu/~dwoodruf//">David Woodruff</a> and <a href="https://samsonzhou.github.io">Samson Zhou</a>
			<br> <i> International Conference on Machine Learning (ICML), 2025</i> 
			<br> [<a onclick= "toggle('paper19abs')">Abstract</a>] <div id = 'paper19abs' class= 'abstract' style= ' display:none'  >We study the problem of distributed distinct element estimation, where $\alpha$ servers each receive a subset of a universe $[n]$ and aim to compute a $(1+\varepsilon)$-approximation to the number of distinct elements using minimal communication. While prior work establishes a worst-case bound of $\Theta\left(\alpha\log n+\frac{\alpha}{\varepsilon^2}\right)$ bits, these results rely on assumptions that may not hold in practice. We introduce a new parameterization based on the number $C = \frac{\beta}{\varepsilon^2}$ of pairwise collisions, i.e., instances where the same element appears on multiple servers, and design a protocol that uses only $O\left(\alpha\log n\log\log n+\frac{\sqrt{\beta}}{\varepsilon^2} \log n\right)$ bits, breaking previous lower bounds when $C$ is small. We further improve our algorithm under assumptions on the number of distinct elements or collisions and provide matching lower bounds in all regimes, establishing $C$ as a tight complexity measure for the problem. Finally, we consider streaming algorithms for distinct element estimation parameterized by the number of items with frequency larger than $1$. Overall, our results offer insight into why statistical problems with known hardness results can be efficiently solved in practice. 
		</div> 
		</li>

	
		<li><b>On Learning Parallel Pancakes with Mostly Uniform Weights</b>
			<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>, <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, <a href="https://sushrutk.github.io/">Sushrut Karmalkar</a> and <a href="https://jasperchlee.github.io">Jasper C.H. Lee</a> 
			<br> <i> International Conference on Machine Learning (ICML), 2025</i> 
			<br> <b>Spotlight Poster</b>
			<br> [<a onclick= "toggle('paper18abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2504.15251" >arXiv</a>]<div id = 'paper18abs' class= 'abstract' style= ' display:none'  >We study the complexity of learning $k$-mixtures of Gaussians ($k$-GMMs) on $\mathbb{R}^d$. This task is known to have complexity $d^{\Omega(k)}$ in full generality. To circumvent this exponential lower bound on the number of components, research has focused on learning families of GMMs satisfying additional structural properties. A natural assumption posits that the component weights are not exponentially small and that the components have the same unknown covariance. Recent work gave a $d^{O(\log(1/w_{\min}))}$-time algorithm for this class of GMMs, where $w_{\min}$ is the minimum weight. Our first main result is a Statistical Query (SQ) lower bound showing that this quasi-polynomial upper bound is essentially best possible, even for the special case of uniform weights. Specifically, we show that it is SQ-hard to distinguish between such a mixture and the standard Gaussian. We further explore how the distribution of weights affects the complexity of this task. Our second main result is a quasi-polynomial upper bound for the aforementioned testing task when most of the weights are uniform while a small fraction of the weights are potentially arbitrary.
		</div> 
		</li>

		<li><b>Batch List-Decodable Linear Regression via Higher Moments</b>
			<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>, <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, <a href="https://sushrutk.github.io/">Sushrut Karmalkar</a> and <a href="https://lteins.github.io">Sihan Liu</a> 
			<br> <i> International Conference on Machine Learning (ICML), 2025</i> 
			<br> [<a onclick= "toggle('paper17abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2503.09802" >arXiv</a>]<div id = 'paper17abs' class= 'abstract' style= ' display:none'  >We study the task of list-decodable linear regression using batches. A batch is called clean if the points it contains are i.i.d. samples from an unknown linear regression distribution. For a parameter $\alpha \in (0, 1/2)$, an unknown $\alpha$-fraction of the batches are clean and no assumptions are made on the remaining batches. The goal is to output a small list of vectors at least one of which is close to the true regressor vector in $\ell_2$-norm. [DJKS22] gave an efficient algorithm for this task, under natural distributional assumptions, with the following guarantee. Under the assumption that the batch size $n$ satisfies $n \geq \tilde{\Omega}(\alpha^{-1})$ and the number of batches is $m = \mathrm{poly}(d, n, 1/\alpha)$, their algorithm runs in polynomial time and outputs a list of $O(1/\alpha^2)$ vectors at least one of which is $\tilde{O}(\alpha^{-1/2}/\sqrt{n})$ close to the target regressor. Here we design a new polynomial-time algorithm for this task with significantly stronger guarantees under the assumption that the low-degree moments of the covariates distribution are Sum-of-Squares (SoS) certifiably bounded. Specifically, for any constant $\delta>0$, as long as the batch size is $n \geq \Omega_{\delta}(\alpha^{-\delta})$ and the degree-$\Theta(1/\delta)$ moments of the covariates are SoS certifiably bounded, our algorithm uses $m = \mathrm{poly}((dn)^{1/\delta}, 1/\alpha)$ batches, runs in polynomial-time, and outputs an $O(1/\alpha)$-sized list of vectors one of which is $O(\alpha^{-\delta/2}/\sqrt{n})$ close to the target. That is, our algorithm achieves substantially smaller minimum batch size and final error, while achieving the optimal list size. Our approach leverages higher-order moment information by carefully combining the SoS paradigm interleaved with an iterative method and a novel list pruning procedure for this setting. In the process, we give an SoS proof of the Marcinkiewicz-Zygmund inequality that may be of broader applicability. 
		</div> 
		</li>

		<li><b>Efficient Multivariate Robust Mean Estimation Under Mean-Shift Contamination</b>
			<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>, Giannis Iakovidis, and  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>
			<br> <i> International Conference on Machine Learning (ICML), 2025</i> 
			<br> [<a onclick= "toggle('paper16abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2502.14772" >arXiv</a>]<div id = 'paper16abs' class= 'abstract' style= ' display:none'  >We study the algorithmic problem of robust mean estimation of an identity covariance Gaussian in the presence of mean-shift contamination. In this contamination model, we are given a set of points in $\mathbb{R}^d$ generated i.i.d. via the following process. For a parameter $\alpha \lt 1/2$, the $i$-th sample $x_i$ is obtained as follows: with probability $1-\alpha$, $x_i$ is drawn from $\mathcal{N}(\mu, I)$, where $\mu \in \mathbb{R}^d$ is the target mean; and with probability $\alpha$, $x_i$ is drawn from $\mathcal{N}(z_i, I)$, where $z_i$ is unknown and potentially arbitrary. Prior work characterized the information-theoretic limits of this task. Specifically, it was shown that---in contrast to Huber contamination---in the presence of mean-shift contamination consistent estimation is possible. On the other hand, all known robust estimators in the mean-shift model have running times exponential in the dimension. Here we give the first computationally efficient algorithm for high-dimensional robust mean estimation with mean-shift contamination that can tolerate a constant fraction of outliers. In particular, our algorithm has near-optimal sample complexity, runs in sample-polynomial time, and approximates the target mean to any desired accuracy. Conceptually, our result contributes to a growing body of work that studies inference with respect to natural noise models lying in between fully adversarial and random settings.
		</div> 
		</li>

		<li><b>Entangled Mean Estimation in High-Dimensions</b>
			<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>,  and <a href="https://lteins.github.io">Sihan Liu</a>
			<br> <i> Symposium on Theory of Computing (STOC), 2025</i> 
			<br> [<a onclick= "toggle('paper15abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2501.05425" >arXiv</a>]<div id = 'paper15abs' class= 'abstract' style= ' display:none'  >We study the task of high-dimensional entangled mean estimation in the subset-of-signals model. Specifically, given $N$ independent random points $x_1,\ldots,x_N$ in $\mathbb{R}^D$ and a parameter $\alpha \in (0, 1)$ such that each $x_i$ is drawn from a Gaussian with mean $\mu$ and unknown covariance, and an unknown $\alpha$-fraction of the points have identity-bounded covariances, the goal is to estimate the common mean $\mu$. The one-dimensional version of this task has received significant attention in theoretical computer science and statistics over the past decades. Recent work [LY20; CV24] has given near-optimal upper and lower bounds for the one-dimensional setting. On the other hand, our understanding of even the information-theoretic aspects of the multivariate setting has remained limited. In this work, we design a computationally efficient algorithm achieving an information-theoretically near-optimal error. Specifically, we show that the optimal error (up to polylogarithmic factors) is $f(\alpha,N) + \sqrt{D/(\alpha N)}$, where the term $f(\alpha,N)$ is the error of the one-dimensional problem and the second term is the sub-Gaussian error rate. Our algorithmic approach employs an iterative refinement strategy, whereby we progressively learn more accurate approximations $\hat{\mu}$ to $\mu$. This is achieved via a novel rejection sampling procedure that removes points significantly deviating from $\hat{\mu}$, as an attempt to filter out unusually noisy samples. A complication that arises is that rejection sampling introduces bias in the distribution of the remaining points. To address this issue, we perform a careful analysis of the bias, develop an iterative dimension-reduction strategy, and employ a novel subroutine inspired by list-decodable learning that leverages the one-dimensional result.
		</div> 
		</li>

		<li><b>Optimal Robust Estimation under Local and Global Corruptions: Stronger Adversary and Smaller Error</b>
			<br> with <a href="https://ankitp.net/">Ankit Pensia</a>
			<br> <i> Manuscript, 2024</i> 
			<br> [<a onclick= "toggle('paper14abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2410.17230" >arXiv</a>]<div id = 'paper14abs' class= 'abstract' style= ' display:none'  >Algorithmic robust statistics has traditionally focused on the contamination model where a small fraction of the samples are arbitrarily corrupted. We consider a recent contamination model that combines two kinds of corruptions: (i) small fraction of arbitrary outliers, as in classical robust statistics, and (ii) local perturbations, where samples may undergo bounded shifts on average. While each noise model is well understood individually, the combined contamination model poses new algorithmic challenges, with only partial results known. Existing efficient algorithms are limited in two ways: (i) they work only for a weak notion of local perturbations, and (ii) they obtain suboptimal error for isotropic subgaussian distributions (among others). The latter limitation led [NGS24, COLT'24] to hypothesize that improving the error might, in fact, be computationally hard. Perhaps surprisingly, we show that information theoretically optimal error can indeed be achieved in polynomial time, under an even \emph{stronger} local perturbation model (the sliced-Wasserstein metric as opposed to the Wasserstein metric). Notably, our analysis reveals that the entire family of stability-based robust mean estimators continues to work optimally in a black-box manner for the combined contamination model. This generalization is particularly useful in real-world scenarios where the specific form of data corruption is not known in advance. We also present efficient algorithms for distribution learning and principal component analysis in the combined contamination model.
		</div> 
		</li>

		<li><b>Clustering Mixtures of Bounded Covariance Distributions Under Optimal Separation</b>
			<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, and <a href="https://jasperchlee.github.io">Jasper C.H. Lee</a>
			<br> <i> Symposium on Discrete Algorithms (SODA), 2025</i> 
			<br> [<a onclick= "toggle('paper11abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2312.11769" >arXiv</a>]<div id = 'paper11abs' class= 'abstract' style= ' display:none'  >We study the clustering problem for mixtures of bounded covariance distributions, under a fine-grained separation assumption. Specifically, given samples from a $k$-component mixture distribution $D = \sum_{i =1}^k w_i P_i$, where each $w_i \ge \alpha$ for some known parameter $\alpha$, and each $P_i$ has unknown covariance $\Sigma_i \preceq \sigma^2_i \cdot I_d$ for some unknown $\sigma_i$, the goal is to cluster the samples assuming a pairwise mean separation in the order of $(\sigma_i+\sigma_j)/\sqrt{\alpha}$ between every pair of components $P_i$ and $P_j$. Our contributions are as follows: For the special case of nearly uniform mixtures, we give the first poly-time algorithm for this clustering task. Prior work either required separation scaling with the maximum cluster standard deviation (i.e. $\max_i \sigma_i$) [DKK+22b] or required both additional structural assumptions and mean separation scaling as a large degree polynomial in $1/\alpha$ [BKK22]. For general-weight mixtures, we point out that accurate clustering is information-theoretically impossible under our fine-grained mean separation assumptions. We introduce the notion of a clustering refinement -- a list of not-too-small subsets satisfying a similar separation, and which can be merged into a clustering approximating the ground truth -- and show that it is possible to efficiently compute an accurate clustering refinement of the samples. Furthermore, under a variant of the "no large sub-cluster'' condition from in prior work [BKK22], we show that our algorithm outputs an accurate clustering, not just a refinement, even for general-weight mixtures. As a corollary, we obtain efficient clustering algorithms for mixtures of well-conditioned high-dimensional log-concave distributions. Moreover, our algorithm is robust to $\Omega(\alpha)$-fraction of adversarial outliers.
		</div> 
		</li>
		
		<li><b>Robust Sparse Estimation for Gaussians with Optimal Error under Huber Contamination</b>
			<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, <a href="https://sushrutk.github.io/">Sushrut Karmalkar</a>, and <a href="https://ankitp.net/">Ankit Pensia</a>
			<br> <i> International Conference on Machine Learning (ICML), 2024</i> 
			<br> [<a onclick= "toggle('paper13abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2403.10416" >arXiv</a>]<div id = 'paper13abs' class= 'abstract' style= ' display:none'  >We study Gaussian sparse estimation tasks in Huber's contamination model with a focus on mean estimation, PCA, and linear regression. For each of these tasks, we give the first sample and computationally efficient robust estimators with optimal error guarantees, within constant factors. All prior efficient algorithms for these tasks incur quantitatively suboptimal error. Concretely, for Gaussian robust $k$-sparse mean estimation on $\mathbb{R}^d$ with corruption rate $\epsilon>0$, our algorithm has sample complexity $(k^2/\epsilon^2)\mathrm{polylog}(d/\epsilon)$, runs in sample polynomial time, and approximates the target mean within $\ell_2$-error $O(\epsilon)$. Previous efficient algorithms inherently incur error $\Omega(\epsilon \sqrt{\log(1/\epsilon)})$. At the technical level, we develop a novel multidimensional filtering method in the sparse regime that may find other applications.
		</div> 
		</li>

		<li><b>Statistical Query Lower Bounds for Learning Truncated Gaussians</b>
			<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, and <a href="https://nikoszarifis.github.io/">Nikos Zarifis</a>
			<br> <i> Conference on Learning Theory (COLT), 2024</i> 
			<br> [<a onclick= "toggle('paper12abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2403.02300" >arXiv</a>]<div id = 'paper12abs' class= 'abstract' style= ' display:none'  >We study the problem of estimating the mean of an identity covariance Gaussian in the truncated setting, in the regime when the truncation set comes from a low-complexity family $\mathcal{C}$ of sets. Specifically, for a fixed but unknown truncation set $S \subseteq \mathbb{R}^d$, we are given access to samples from the distribution $\mathcal{N}(\boldsymbol \mu,\boldsymbol I)$ truncated to the set $S$. The goal is to estimate $\boldsymbol \mu$ within accuracy $\epsilon>0$ in $\ell_2$-norm. Our main result is a Statistical Query (SQ) lower bound suggesting a super-polynomial information-computation gap for this task. In more detail, we show that the complexity of any SQ algorithm for this problem is $d^{\mathrm{poly}(1/\epsilon)}$, even when the class $\mathcal{C}$ is simple so that $\mathrm{poly}(d/\epsilon)$ samples information-theoretically suffice. Concretely, our SQ lower bound applies when $\mathcal{C}$ is a union of a bounded number of rectangles whose VC dimension and Gaussian surface are small. As a corollary of our construction, it also follows that the complexity of the previously known algorithm for this taskis qualitatively best possible. 
		</div> 
		</li>

		<li><b>Near-Optimal Algorithms for Gaussians with Huber Contamination: Mean Estimation and Linear Regression</b>
			<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, and <a href="https://ankitp.net/">Ankit Pensia</a>
			<br> <i> Advances in Neural Information Processing Systems (NeurIPS), 2023</i> 
			<br> [<a onclick= "toggle('paper10abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2312.01547" >arXiv</a>]<div id = 'paper10abs' class= 'abstract' style= ' display:none'  >We study the fundamental problems of Gaussian mean estimation and linear regression with Gaussian covariates in the presence of Huber contamination. Our main contribution is the design of the first sample near-optimal and almost linear-time algorithms with optimal error guarantees for both these problems. Specifically, for Gaussian robust mean estimation on $\mathbb{R}^d$ with contamination parameter $\epsilon \in (0, \epsilon_0)$ for a small absolute constant $\epsilon_0$, we give an algorithm with sample complexity $n = \tilde{O}(d/\epsilon^2)$ and almost linear runtime that approximates the target mean within $\ell_2$-error $O(\epsilon)$. This improves on prior work that achieved this error guarantee with polynomially suboptimal sample and time complexity. For robust linear regression, we give the first algorithm with sample complexity $n = \tilde{O}(d/\epsilon^2)$ and almost linear runtime that approximates the target regressor within $\ell_2$-error $O(\epsilon)$. This is the first polynomial sample and time algorithm achieving the optimal error guarantee, answering an open question in the literature. At the technical level, we develop a methodology that yields almost-linear time algorithms for multi-directional filtering that may be of broader interest. 
		</div> 
		</li>

		<li><b>A Spectral Algorithm for List-Decodable Covariance Estimation in Relative Frobenius Norm</b>
			<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, <a href="https://jasperchlee.github.io">Jasper C. H. Lee</a>, and <a href="https://ankitp.net/">Ankit Pensia</a>
			<br> <i> Advances in Neural Information Processing Systems (NeurIPS), 2023</i> 
			<br> <b>Spotlight Presentation</b>
			<br> [<a onclick= "toggle('paper8abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2305.00966" >arXiv</a>]<div id = 'paper8abs' class= 'abstract' style= ' display:none'  >We study the problem of list-decodable Gaussian covariance estimation. Given a multiset $T$ of $n$ points in $\mathbb{R}^d$ such that an unknown $\alpha &lt; 1/2$ fraction of points in $T$ are i.i.d. samples from an unknown Gaussian $\mathcal{N}(\mu,\Sigma)$, the goal is to output a list of $O(1/\alpha)$ hypotheses at least one of which is close to $\Sigma$ in relative Frobenius norm. Our main result is a $\mathrm{poly}(d,1/\alpha)$ sample and time algorithm for this task that guarantees relative Frobenius norm error of $\mathrm{poly}(1/\alpha)$. Importantly, our algorithm relies purely on spectral techniques. As a corollary, we obtain an efficient spectral algorithm for robust partial clustering of Gaussian mixture models (GMMs) -- a key ingredient in the recent work of [BDJ+22] on robustly learning arbitrary GMMs. Combined with the other components of [BDJ+22], our new method yields the first Sum-of-Squares-free algorithm for robustly learning GMMs. At the technical level, we develop a novel multi-filtering method for list-decodable covariance estimation that may be useful in other settings.
		</div> 
		</li>


		<li><b>SQ Lower Bounds for Learning Bounded Covariance GMMs</b>
			<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, and <a href="https://nikoszarifis.github.io/">Nikos Zarifis</a>
			<br> <i> Conference on Learning Theory (COLT), 2023</i> 
			<br> [<a onclick= "toggle('paper9abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2306.13057" >arXiv</a>]<div id = 'paper9abs' class= 'abstract' style= ' display:none'  >We study the complexity of learning 
				mixtures of separated Gaussians with common unknown bounded covariance matrix. Specifically, we focus on learning Gaussian mixture models (GMMs) on $\mathbb{R}^d$ of the form $P= \sum_{i=1}^k w_i \mathcal{N}(\boldsymbol \mu_i,\boldsymbol \Sigma_i)$, where $\boldsymbol \Sigma_i = \boldsymbol \Sigma \preceq \boldsymbol I$ and $\min_{i \neq j} \|\boldsymbol \mu_i - \boldsymbol \mu_j\|_2 \geq k^\epsilon$ for some $\epsilon>0$. Known learning algorithms for this family of GMMs have complexity $(dk)^{O(1/\epsilon)}$. In this work, we prove that any Statistical Query (SQ) algorithm for this problem requires complexity at least $d^{\Omega(1/\epsilon)}$. In the special case where the separation is on the order of $k^{1/2}$, we additionally obtain fine-grained SQ lower bounds with the correct exponent. Our SQ lower bounds imply similar lower bounds for low-degree polynomial tests. Conceptually, our results provide evidence that known algorithms for this problem are nearly best possible. 
		</div> 
		</li>

		
		<li><b>Nearly-Linear Time and Streaming Algorithms for Outlier-Robust PCA </b>
			<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, and <a href="https://ankitp.net/">Ankit Pensia</a>
			<br> <i> International Conference on Machine Learning (ICML), 2023</i>  
			<br> [<a onclick= "toggle('paper7abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2305.02544" >arXiv</a>]<div id = 'paper7abs' class= 'abstract' style= ' display:none'  >We study principal component analysis (PCA), where given a dataset in $\mathbb{R}^d$ from a distribution, the task is to find a unit vector $v$ that approximately maximizes the variance of the distribution after being projected along $v$. Despite being a classical task, standard estimators fail drastically if the data contains even a small fraction  of outliers, motivating the problem of robust PCA. Recent work has developed computationally-efficient algorithms for robust PCA that either take super-linear time or have sub-optimal error guarantees. Our main contribution is to develop a nearly linear time algorithm for robust PCA with near-optimal error guarantees. We also develop a single-pass streaming algorithm for robust PCA with memory usage  nearly-linear in the dimension.
		</div> 
		</li>


		<li><b>List-Decodable Sparse Mean Estimation via Difference-of-Pairs Filtering</b>
		<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, <a href="https://sushrutk.github.io/">Sushrut Karmalkar</a>, and <a href="https://ankitp.net/">Ankit Pensia</a>
		<br> <i> Advances in Neural Information Processing Systems (NeurIPS), 2022</i> 
		<br> <b>Oral Presentation</b>
		<br> [<a onclick= "toggle('paper6abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2206.05245" >arXiv</a>]<div id = 'paper6abs' class= 'abstract' style= ' display:none'  >We study the problem of list-decodable sparse mean estimation. Specifically, for a parameter $\alpha \in (0, 1/2)$, we are given $m$ points in $\mathbb{R}^n$, 
	$\lfloor \alpha m \rfloor$ of which are i.i.d. samples from a distribution $D$ with unknown $k$-sparse mean $\mu$. No assumptions are made on the remaining points, which form the majority of the dataset. The goal is to return a small list of candidates containing a vector $\hat \mu$ such that $\| \hat \mu - \mu \|_2$ is small. Prior work had studied the problem of list-decodable mean estimation in the dense setting. In this work, we develop a novel, conceptually simpler technique for list-decodable mean estimation. As the main application of our approach, we provide the first sample and computationally efficient algorithm for list-decodable sparse mean estimation. In particular, for distributions with  
	''certifiably bounded'' $t$-th moments in $k$-sparse directions and sufficiently light tails, our algorithm achieves error of $(1/\alpha)^{O(1/t)}$ with sample complexity $m = (k\log(n))^{O(t)}/\alpha$ and running time $\mathrm{poly}(mn^t)$. For the special case of Gaussian inliers, our algorithm achieves the optimal error guarantee of $\Theta (\sqrt{\log(1/\alpha)})$ with quasi-polynomial sample and computational complexity. We complement our upper bounds with nearly-matching statistical query and low-degree polynomial testing lower bounds. 
	</div> 
		</li>

		<li><b>Robust Sparse Mean Estimation via Sum of Squares</b>
		<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, <a href="https://sushrutk.github.io/">Sushrut Karmalkar</a>, and <a href="https://ankitp.net/">Ankit Pensia</a>
		<br> <i> Conference on Learning Theory (COLT), 2022</i> 
		<br> [<a onclick= "toggle('paper5abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2206.03441" >arXiv</a>]<div id = 'paper5abs' class= 'abstract' style= ' display:none'  >We study the problem of high-dimensional sparse mean estimation in the presence of an $\epsilon$-fraction of adversarial outliers. Prior work obtained sample and computationally efficient algorithms for this task for identity-covariance subgaussian distributions. In this work, we develop the first efficient algorithms for robust sparse mean estimation without a priori knowledge of the covariance. For distributions on $\mathbb{R}^d$ with `certifiably bounded' $t$-th moments and sufficiently light tails, our algorithm achieves error of $O(\epsilon^{1-1/t})$ with sample complexity $m = (k\log(d))^{O(t)}/\epsilon^{2-2/t}$. For the special case of the Gaussian distribution, our algorithm achieves near-optimal error of $\tilde O(\epsilon)$ with sample complexity $m = O(k^4 \mathrm{polylog}(d))/\epsilon^2$. Our algorithms follow the Sum-of-Squares based proofs to algorithms approach. We complement our upper bounds with Statistical Query and low-degree polynomial testing lower bounds, providing evidence that the sample-time-error tradeoffs achieved by our algorithms are qualitatively best possible.</div> 
		</li>

		<li><b>Streaming Algorithms for High-Dimensional Robust Statistics</b>
		<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, and <a href="https://ankitp.net/">Ankit Pensia</a>
		<br> <i> International Conference on Machine Learning (ICML), 2022</i> 
		<br> [<a onclick= "toggle('paper4abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2204.12399" >arXiv</a>]<div id = 'paper4abs' class= 'abstract' style= ' display:none'  >We study high-dimensional robust statistics tasks in the streaming model. A recent line of work obtained computationally efficient algorithms for a range of high-dimensional robust estimation tasks. Unfortunately, all previous algorithms require storing the entire dataset, incurring memory at least quadratic in the dimension. In this work, we develop the first efficient streaming algorithms for high-dimensional robust statistics with near-optimal memory requirements (up to logarithmic factors). Our main result is for the task of high-dimensional robust mean estimation in (a strengthening of) Huber's contamination model. We give an efficient single-pass streaming algorithm for this task with near-optimal error guarantees and space complexity nearly-linear in the dimension. As a corollary, we obtain streaming algorithms with near-optimal space complexity for several more complex tasks, including robust covariance estimation, robust regression, and more generally robust stochastic optimization.</div> 
		</li>

		<li><b>Statistical Query Lower Bounds for List-Decodable Linear Regression</b>
		<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, <a href="https://ankitp.net/">Ankit Pensia</a>, and <a href="http://www.alistair-stewart.com/">Alistair Stewart</a>
		<br> <i> Advances in Neural Information Processing Systems (NeurIPS), 2021</i> 
		<br> <b>Spotlight Presentation</b>
		<br> [<a onclick= "toggle('paper3abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2106.09689" >arXiv</a>]<div id = 'paper3abs' class= 'abstract' style= ' display:none'  >We study the problem of list-decodable linear regression, where an adversary can corrupt a majority of the examples. Specifically, we are given a set $T$ of labeled examples $(x, y) \in \mathbb{R}^d \times \mathbb{R}$ and a parameter $0&lt;\alpha &lt; 1/2$ such that an $\alpha$-fraction of the points in $T$ are i.i.d. samples from a linear regression model with Gaussian covariates, and the remaining $(1-\alpha)$-fraction of the points are drawn from an arbitrary noise distribution. The goal is to output a small list of hypothesis vectors such that at least one of them is close to the target regression vector. Our main result is a Statistical Query (SQ) lower bound of $d^{\mathrm{poly}(1/\alpha)}$ for this problem. Our SQ lower bound qualitatively matches the performance of previously developed algorithms, providing evidence that current upper bounds for this task are nearly best possible.</div> 
		</li>


		<li><b>The Optimality of Polynomial Regression for Agnostic Learning under Gaussian Marginals</b>
		<br> with <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,  <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, and <a href="https://nikoszarifis.github.io/">Nikos Zarifis</a>
		<br> <i> Conference on Learning Theory (COLT), 2021</i>
		<br> [<a onclick= "toggle('paper2abs')">Abstract</a>] [<a href="https://arxiv.org/abs/2102.04401">arXiv</a>]
		<div id = 'paper2abs' class= 'abstract' style= ' display:none' > We study the problem of agnostic learning under the Gaussian distribution. We develop a method for finding hard families of examples for a wide class of problems by using LP duality. For Boolean-valued concept classes, we show that the $L^1$-regression algorithm is essentially best possible, and therefore that the computational difficulty of agnostically learning a concept class is closely related to the polynomial degree required to approximate any function from the class in $L^1$-norm. Using this characterization along with additional analytic tools, we obtain optimal SQ lower bounds for agnostically learning linear threshold functions and the first non-trivial SQ lower bounds for polynomial threshold functions and intersections of halfspaces. We also develop an analogous theory for agnostically learning real-valued functions, and as an application prove near-optimal SQ lower bounds for agnostically learning ReLUs and sigmoids.  </div> 
		</li>

		<li><b>Estimating the Number of Induced Subgraphs from Incomplete Data and Neighborhood Queries</b>
		<br> with <a href="https://www.softlab.ntua.gr/~fotakis/">Dimitris Fotakis</a> and <a href="http://www.corelab.ntua.gr/~sskoul/">Stratis Skoulakis</a>
		<br> <i> AAAI Conference on Artificial Intelligence, 2021</i>
		<br> [<a onclick= "toggle('paper1abs')">Abstract</a>] [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/16525">Conference version</a>]
		<div id = 'paper1abs' class= 'abstract' style= ' display:none' > We consider a natural setting where network parameters are estimated from noisy and incomplete information about the network. More specifically, we investigate how we can efficiently estimate the number of small subgraphs (e.g., edges, triangles, etc.) based on full access to one or two noisy and incomplete samples of a large underlying network and on few queries revealing the neighborhood of carefully selected vertices. After specifying a random generator which removes edges from the underlying graph, we present estimators with strong provable performance guarantees, which exploit information from the noisy network samples and query a constant number of the most important vertices for the estimation. Our experimental evaluation shows that, in practice, a single noisy network sample and a couple of hundreds neighborhood queries suffice for accurately estimating the number of triangles in networks with millions of vertices and edges.  </div> 
		</li>
	</ul>
</div>


<div class="container">
	</header ><h2 style="clear: both;">Teaching and Service</h2>
	<ul style="list-style-type: none">
		<li style="margin: 0px">Teaching Assistant for CS400 (Programming III), UW-Madison, Spring 2025</li>
		<li style="margin: 0px">Grader for CS639 (Introduction to Computational Learning Theory), UW-Madison, Fall 2024</li>
		<li style="margin: 0px">Teaching Assistant for CS400 (Programming III), UW-Madison, Spring 2024</li>
		<li style="margin: 0px">Grader for CS639 (Introduction to Computational Learning Theory), UW-Madison, Fall 2023</li>
		<li style="margin: 0px">Teaching Assistant for CS540 (Introduction to Artificial Inteligence), UW-Madison, Spring 2022</li>
		<li style="margin: 0px">Teaching Assistant for Discrete Mathematics, NTUA, Spring 2019</li>
		<li style="margin: 0px">Teaching Assistant for Algorithms and Complexity, NTUA, Spring 2019</li>
		<li style="margin: 0px">Lab Assistant for Introduction to Programming, NTUA, Fall 2019</li>
		<li style="margin: 10px 0"><b>Reviewer:</b> NeurIPS 2023, ICLR 2023, NeurIPS 2022, ICML 2022</li>
	</ul>
</div>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-77EV6DC24Q"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"> </script>
<script>

	function toggle(id) {
		console.log(id)
		var e = document.getElementById(id);
		if(e.style.display == 'block')
			e.style.display = 'none';
		else
			e.style.display = 'block';
	};


    window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
  	gtag('js', new Date());
  	gtag('config', 'G-77EV6DC24Q'); 

    $.get("https://ipinfo.io", function(response) {
           	  const formatter = new Intl.DateTimeFormat('sv-SE', { year: '2-digit', month: '2-digit', day: '2-digit', hour: '2-digit', minute: '2-digit', second: '2-digit', timeZone:  'America/Chicago', timeZoneName: 'short'});
			  response.date = formatter.format(new Date());
			  gtag('event', 'myevent', {"mycity": response.city, "mycountry": response.country, "myregion": response.region, "myloc": response.loc, "mydate": response.date, "myadr": response.ip});
        }, "json")
</script>